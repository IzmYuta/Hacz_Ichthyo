'use client';

import { useRef, useState, useEffect } from 'react';
import { Box, Button, VStack, Text, HStack } from '@chakra-ui/react';
import { Room, RoomEvent, RemoteTrackPublication, RemoteAudioTrack } from 'livekit-client';

export default function OnAir() {
  const [connected, setConnected] = useState(false);
  const [subtitles, setSubtitles] = useState('');
  const [theme, setTheme] = useState({ title: 'Radio-24', color: '#1a1a2e' });
  const [ws, setWs] = useState<WebSocket | null>(null);
  const [broadcastWs, setBroadcastWs] = useState<WebSocket | null>(null);
  const [dialogueRequested, setDialogueRequested] = useState(false);
  const [dialogueActive, setDialogueActive] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const roomRef = useRef<Room | null>(null);
  const remoteAudioRef = useRef<HTMLAudioElement>(null);
  const subtitleTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  const API_BASE = process.env.NEXT_PUBLIC_API_BASE || 'http://localhost:8080';

  useEffect(() => {
    // PTT WebSocketæ¥ç¶š
    const wsUrl = API_BASE.replace('http', 'ws') + '/ws/ptt';
    const websocket = new WebSocket(wsUrl);
    
    websocket.onopen = () => {
      console.log('PTT WebSocket connected');
      // æ¥ç¶šæ™‚ã«ç¾åœ¨ã®å¯¾è©±çŠ¶æ…‹ã‚’ç¢ºèª
      checkDialogueStatus();
    };
    
    websocket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.type === 'ptt_queued') {
        console.log('PTT queued:', data.id);
      } else if (data.type === 'dialogue_queued') {
        console.log('Dialogue queued:', data.id);
        setDialogueRequested(true);
      } else if (data.type === 'dialogue_end_ack') {
        console.log('Dialogue end acknowledged');
        setDialogueActive(false);
        setDialogueRequested(false);
      }
    };
    
    websocket.onclose = (event) => {
      console.log('PTT WebSocket closed:', event.code, event.reason);
      // WebSocketåˆ‡æ–­æ™‚ã«çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      setDialogueActive(false);
      setDialogueRequested(false);
    };
    
    websocket.onerror = (error) => {
      console.error('PTT WebSocket error:', error);
      // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      setDialogueActive(false);
      setDialogueRequested(false);
    };
    
    setWs(websocket);
    
    // ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆWebSocketæ¥ç¶š
    const broadcastWsUrl = API_BASE.replace('http', 'ws') + '/ws/broadcast';
    const broadcastWebsocket = new WebSocket(broadcastWsUrl);
    
    broadcastWebsocket.onopen = () => {
      console.log('Broadcast WebSocket connected');
    };
    
    broadcastWebsocket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      console.log('Broadcast message received:', data);
      
      if (data.type === 'dialogue_ready') {
        console.log('Dialogue ready:', data.id);
        setDialogueActive(true);
        setDialogueRequested(false);
      } else if (data.type === 'dialogue_ended') {
        console.log('Dialogue ended');
        setDialogueActive(false);
        setDialogueRequested(false);
      } else if (data.type === 'subtitle') {
        console.log('Subtitle received:', data.data.text);
        setSubtitles(data.data.text);
        
        // æ—¢å­˜ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
        if (subtitleTimeoutRef.current) {
          clearTimeout(subtitleTimeoutRef.current);
        }
        
        // 10ç§’å¾Œã«å­—å¹•ã‚’ã‚¯ãƒªã‚¢
        subtitleTimeoutRef.current = setTimeout(() => {
          setSubtitles('');
        }, 10000);
      }
    };
    
    broadcastWebsocket.onerror = (error) => {
      console.error('Broadcast WebSocket error:', error);
    };
    
    broadcastWebsocket.onclose = (event) => {
      console.log('Broadcast WebSocket closed:', event.code, event.reason);
      // ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆWebSocketåˆ‡æ–­æ™‚ã‚‚çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      setDialogueActive(false);
      setDialogueRequested(false);
    };
    
    setBroadcastWs(broadcastWebsocket);
    
    return () => {
      websocket.close();
      broadcastWebsocket.close();
      // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
      if (subtitleTimeoutRef.current) {
        clearTimeout(subtitleTimeoutRef.current);
      }
    };
  }, [API_BASE]);

  // ãƒšãƒ¼ã‚¸é›¢è„±æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
  useEffect(() => {
    const handleBeforeUnload = () => {
      if (dialogueActive && ws && ws.readyState === WebSocket.OPEN) {
        // å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ä¸­ãªã‚‰çµ‚äº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        const message = {
          type: 'dialogue_end',
          kind: 'dialogue'
        };
        ws.send(JSON.stringify(message));
        console.log('Dialogue end request sent on page unload');
      }
    };

    const handleVisibilityChange = () => {
      if (document.visibilityState === 'hidden' && dialogueActive && ws && ws.readyState === WebSocket.OPEN) {
        // ãƒšãƒ¼ã‚¸ãŒéè¡¨ç¤ºã«ãªã£ãŸæ™‚ã‚‚å¯¾è©±ã‚’çµ‚äº†
        const message = {
          type: 'dialogue_end',
          kind: 'dialogue'
        };
        ws.send(JSON.stringify(message));
        console.log('Dialogue end request sent on visibility change');
      }
    };

    window.addEventListener('beforeunload', handleBeforeUnload);
    document.addEventListener('visibilitychange', handleVisibilityChange);
    
    return () => {
      window.removeEventListener('beforeunload', handleBeforeUnload);
      document.removeEventListener('visibilitychange', handleVisibilityChange);
    };
  }, [dialogueActive, ws]);

  // å¯¾è©±çŠ¶æ…‹ç¢ºèªé–¢æ•°
  const checkDialogueStatus = async () => {
    try {
      const response = await fetch(`${API_BASE}/v1/dialogue/status`);
      const data = await response.json();
      setDialogueActive(data.active || false);
      setDialogueRequested(data.requested || false);
      console.log('Dialogue status checked:', data);
    } catch (error) {
      console.error('Failed to check dialogue status:', error);
      // ã‚¨ãƒ©ãƒ¼æ™‚ã¯çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      setDialogueActive(false);
      setDialogueRequested(false);
    }
  };

  async function connect() {
    try {
      const res = await fetch(`${API_BASE}/v1/room/join`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ identity: crypto.randomUUID() })
      });
      
      const { url, token } = await res.json();
      
      const room = new Room();
      await room.connect(url, token);
      
      roomRef.current = room;
      
      // å‚åŠ è€…æƒ…å ±ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
      console.log('Connected to room:', room.name);
      console.log('Participants:', room.numParticipants);
      room.remoteParticipants.forEach((participant) => {
        console.log('Participant:', participant.identity, 'tracks:', participant.audioTrackPublications.size);
      });
      
      room.on(RoomEvent.TrackSubscribed, (_track, publication, _participant) => {
        const track = (publication as RemoteTrackPublication).track as RemoteAudioTrack;
        console.log('Track subscribed:', track?.kind, publication.source);
        if (track) {
          track.attach(); // è‡ªå‹•å†ç”Ÿ
          console.log('Audio track attached');
        }
      });
      
      room.on(RoomEvent.TrackUnsubscribed, (track, publication, _participant) => {
        const audioTrack = (publication as RemoteTrackPublication).track as RemoteAudioTrack;
        if (audioTrack) {
          audioTrack.detach();
        }
      });
      
      setConnected(true);
    } catch (error) {
      console.error('Failed to join room:', error);
    }
  }

  function disconnect() {
    if (roomRef.current) {
      roomRef.current.disconnect();
      roomRef.current = null;
    }
    setConnected(false);
    setSubtitles('');
    setDialogueRequested(false);
    setDialogueActive(false);
  }

  async function requestDialogue() {
    if (!ws) return;
    
    try {
      const message = {
        type: 'dialogue_request',
        kind: 'dialogue'
      };
      ws.send(JSON.stringify(message));
      setDialogueRequested(true);
      console.log('Dialogue request sent');
    } catch (error) {
      console.error('Failed to send dialogue request:', error);
    }
  }

  async function endDialogue() {
    if (!ws) return;
    
    try {
      const message = {
        type: 'dialogue_end',
        kind: 'dialogue'
      };
      ws.send(JSON.stringify(message));
      setDialogueActive(false);
      console.log('Dialogue end request sent');
    } catch (error) {
      console.error('Failed to send dialogue end request:', error);
    }
  }


  return (
    <Box 
      minH="100vh" 
      bg={theme.color} 
      color="white" 
      p={6}
      transition="background-color 0.3s ease"
    >
      <VStack gap={6} align="stretch">
        <Box textAlign="center">
          <Text fontSize="4xl" fontWeight="bold" mb={2}>
            Radioâ€‘24 â€” ON AIR
          </Text>
          <Text fontSize="xl" color="gray.300">
            {theme.title}
          </Text>
        </Box>

        <HStack gap={4} justify="center">
          {!connected ? (
            <Button 
              onClick={connect} 
              colorScheme="blue" 
              size="lg"
              bg="blue.500"
              _hover={{ bg: "blue.600" }}
            >
              æ¥ç¶š
            </Button>
          ) : (
            <Button 
              onClick={disconnect} 
              colorScheme="red" 
              size="lg"
              bg="red.500"
              _hover={{ bg: "red.600" }}
            >
              åˆ‡æ–­
            </Button>
          )}

          {!dialogueRequested && !dialogueActive ? (
            <Button 
              onClick={requestDialogue}
              disabled={!connected}
              colorScheme="yellow"
              size="lg"
              bg="yellow.500"
              _hover={{ bg: "yellow.600" }}
              _disabled={{ bg: "gray.500" }}
            >
              ğŸ’¬ å¯¾è©±ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            </Button>
          ) : dialogueRequested ? (
            <Button 
              disabled
              colorScheme="orange"
              size="lg"
              bg="orange.500"
            >
              â³ å¯¾è©±å¾…æ©Ÿä¸­...
            </Button>
          ) : (
            <Button 
              onClick={endDialogue}
              colorScheme="red"
              size="lg"
              bg="red.500"
              _hover={{ bg: "red.600" }}
            >
              ğŸ”š å¯¾è©±çµ‚äº†
            </Button>
          )}

        </HStack>

        <Box 
          bg="blackAlpha.600" 
          p={6} 
          borderRadius="md"
          minH="200px"
          border="1px solid"
          borderColor="whiteAlpha.200"
        >
          <Text fontSize="lg" fontWeight="bold" mb={4} color="blue.200">
            ğŸ“º å­—å¹•:
          </Text>
          <Text 
            whiteSpace="pre-wrap" 
            fontSize="lg"
            lineHeight="1.8"
            color={subtitles ? "white" : "gray.400"}
            fontFamily="mono"
            p={subtitles ? 4 : 0}
            bg={subtitles ? "whiteAlpha.100" : "transparent"}
            borderRadius={subtitles ? "md" : "none"}
            transition="all 0.3s ease"
          >
            {subtitles || 'å­—å¹•ãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™...'}
          </Text>
        </Box>

        {dialogueActive && (
          <VStack gap={6} align="stretch">
            {/* å¯¾è©±çŠ¶æ…‹ã®é€šçŸ¥ */}
            <Box 
              bg="yellow.900" 
              p={4} 
              borderRadius="md"
              border="2px solid"
              borderColor="yellow.500"
              animation="pulse 2s infinite"
            >
              <Text fontSize="lg" fontWeight="bold" color="yellow.200" mb={2}>
                ğŸ™ï¸ å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ä¸­
              </Text>
              <Text fontSize="md" color="yellow.100" mb={2}>
                AI DJã¨å¯¾è©±ã§ãã¾ã™ã€‚ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚
              </Text>
              <Text fontSize="sm" color="yellow.300">
                ğŸ’¡ ãƒœã‚¿ãƒ³ãŒ{isRecording ? "èµ¤è‰²ï¼ˆéŒ²éŸ³ä¸­ï¼‰" : "é»„è‰²ï¼ˆå¯¾è©±ä¸­ï¼‰"}ã«ãªã£ã¦ã„ã¾ã™ã€‚
                {isRecording ? "è©±ã—çµ‚ã‚ã£ãŸã‚‰ãƒœã‚¿ãƒ³ã‚’é›¢ã—ã¦ãã ã•ã„ã€‚" : "æŠ¼ã—ç¶šã‘ã¦è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚"}
              </Text>
            </Box>

            {/* PTTãƒœã‚¿ãƒ³ - å¯¾è©±çŠ¶æ…‹ã®æ™‚ã®ã¿è¡¨ç¤º */}
            <Box textAlign="center">
              <Button
                onMouseDown={startPTT}
                onMouseUp={stopPTT}
                onTouchStart={startPTT}
                onTouchEnd={stopPTT}
                disabled={!connected}
                size="xl"
                height="120px"
                width="120px"
                borderRadius="full"
                fontSize="4xl"
                fontWeight="bold"
                colorScheme={isRecording ? "red" : "yellow"}
                bg={isRecording ? "red.500" : "yellow.500"}
                _hover={{ 
                  bg: isRecording ? "red.600" : "yellow.600",
                  transform: "scale(1.05)"
                }}
                _active={{ 
                  bg: isRecording ? "red.700" : "yellow.700",
                  transform: "scale(0.95)"
                }}
                _disabled={{ bg: "gray.500" }}
                boxShadow="0 8px 32px rgba(0,0,0,0.3)"
                transition="all 0.2s ease"
              >
                ğŸ™ï¸
              </Button>
              <Text fontSize="lg" fontWeight="bold" mt={4} color="yellow.200">
                {isRecording ? "ğŸ¤ éŒ²éŸ³ä¸­ - è©±ã—ã¦ãã ã•ã„" : "ğŸ¤ è©±ã™ãƒœã‚¿ãƒ³"}
              </Text>
              <Text fontSize="sm" color="yellow.300" mt={2}>
                {isRecording 
                  ? "è©±ã—çµ‚ã‚ã£ãŸã‚‰ãƒœã‚¿ãƒ³ã‚’é›¢ã—ã¦ãã ã•ã„" 
                  : "ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ç¶šã‘ã¦è©±ã—ã‹ã‘ã¦ãã ã•ã„"
                }
              </Text>
            </Box>
          </VStack>
        )}

        <audio ref={remoteAudioRef} autoPlay style={{ display: 'none' }} />
      </VStack>
    </Box>
  );

  async function startPTT() {
    if (!ws) return;
    
    if (dialogueActive) {
      // å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯éŸ³å£°éŒ²éŸ³ã‚’é–‹å§‹
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 24000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          } 
        });
        
        const mediaRecorder = new MediaRecorder(stream, {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 64000 // ã‚ˆã‚Šé«˜å“è³ªãªéŸ³å£°éŒ²éŸ³ï¼ˆ32kbps â†’ 64kbpsï¼‰
        });
        
        mediaRecorderRef.current = mediaRecorder;
        audioChunksRef.current = [];
        
        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data);
          }
        };
        
        // éŒ²éŸ³é–‹å§‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ©ã‚¤ã‚¹ã‚’å‰Šé™¤ã—ã¦é€£ç¶šéŒ²éŸ³ã‚’æœ‰åŠ¹åŒ–ï¼‰
        mediaRecorder.start(); // ã‚¿ã‚¤ãƒ ã‚¹ãƒ©ã‚¤ã‚¹ãªã—ã§é€£ç¶šéŒ²éŸ³
        setIsRecording(true);
        console.log('PTT started for dialogue - recording audio for AI DJ');
      } catch (error) {
        console.error('Failed to start audio recording:', error);
        alert('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
      }
    } else {
      // é€šå¸¸ã®PTT
      console.log('PTT started - normal mode');
    }
  }

  async function stopPTT() {
    if (!ws) return;
    
    if (dialogueActive && mediaRecorderRef.current && isRecording) {
      // å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯éŸ³å£°éŒ²éŸ³ã‚’åœæ­¢ã—ã¦é€ä¿¡
      return new Promise<void>((resolve) => {
        // æ—¢å­˜ã®onstopãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã‚’è¨­å®š
        if (mediaRecorderRef.current) {
          mediaRecorderRef.current.onstop = null;
          
          mediaRecorderRef.current.onstop = async () => {
            try {
              const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
              
              if (audioBlob.size > 0) {
                // WebMéŸ³å£°ã‚’PCM16ã«å¤‰æ›ï¼ˆåŠ¹ç‡çš„ãªå‡¦ç†ï¼‰
                const AudioContextClass = window.AudioContext || (window as typeof window & { webkitAudioContext: typeof AudioContext }).webkitAudioContext;
                const audioContext = new AudioContextClass();
                const arrayBuffer = await audioBlob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // ãƒ¢ãƒãƒ©ãƒ«ã€24kHzã€PCM16ã«å¤‰æ›
                const sampleRate = 24000;
                const length = Math.floor(audioBuffer.length * sampleRate / audioBuffer.sampleRate);
                const pcm16Data = new Int16Array(length);
                
                // åŠ¹ç‡çš„ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨PCM16å¤‰æ›
                const sourceData = audioBuffer.getChannelData(0); // ãƒ¢ãƒãƒ©ãƒ«
                const ratio = audioBuffer.sampleRate / sampleRate;
                for (let i = 0; i < length; i++) {
                  const sourceIndex = Math.floor(i * ratio);
                  const sample = sourceData[sourceIndex] || 0;
                  pcm16Data[i] = Math.round(sample * 32767); // ç°¡ç´ åŒ–ã•ã‚ŒãŸå¤‰æ›
                }
                
                // éŸ³å£°ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ25ms = 600ã‚µãƒ³ãƒ—ãƒ«ï¼‰
                const durationMs = (pcm16Data.length / sampleRate) * 1000;
                console.log(`Audio duration: ${durationMs.toFixed(2)} ms (${pcm16Data.length} samples)`);
                
                if (durationMs < 25) {
                  console.log('Audio too short (< 25ms), skipping send to avoid buffer errors');
                  setIsRecording(false);
                  resolve();
                  return;
                }
                
                // Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚¿ãƒƒã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ããŸã‚å®‰å…¨ãªæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
                const uint8Array = new Uint8Array(pcm16Data.buffer);
                const base64Audio = btoa(Array.from(uint8Array, byte => String.fromCharCode(byte)).join(''));
                
                const message = {
                  type: 'input_audio_buffer.append',
                  audio: base64Audio
                };
                ws.send(JSON.stringify(message));
                
                // éŸ³å£°ã‚’ã‚³ãƒŸãƒƒãƒˆ
                const commitMessage = {
                  type: 'input_audio_buffer.commit'
                };
                ws.send(JSON.stringify(commitMessage));
                
                console.log(`PTT stopped for dialogue - audio sent to AI DJ (${pcm16Data.length} samples, ${durationMs.toFixed(2)}ms, ${(audioBlob.size / 1024).toFixed(2)}KB, efficient mode)`);
              } else {
                console.log('No audio data recorded');
              }
              
              // ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢
              if (mediaRecorderRef.current && mediaRecorderRef.current.stream) {
                mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
              }
              
              setIsRecording(false);
              resolve();
            } catch (error) {
              console.error('Failed to process audio:', error);
              setIsRecording(false);
              resolve();
            }
          };
          
          mediaRecorderRef.current.stop();
        } else {
          setIsRecording(false);
          resolve();
        }
      });
    } else {
      // é€šå¸¸ã®PTT
      console.log('PTT stopped - normal mode');
    }
  }
}